import torch
import numpy as np
from utils.graph_utils import plot_precision_recall_curve, plot_class_report, plot_confusion_matrix
from utils.inference_utils import _infer_batch
from utils.log_utils import info, debug
from utils.metrics_utils import _compute_metrics, _calculate_average_losses


def evaluate_model(
        model,
        loader,
        criterion,
        device,
        config_settings,
        compute_metrics=False
):
    """
    Method to orchestrate the model evaluation on a loader.
    :param model: The model to evaluate.
    :param loader: The loader on which to evaluate the model.
    :param criterion: The loss function.
    :param device: Device to use.
    :param config_settings: The configuration settings.
    :param compute_metrics: Whether to compute metrics or not.
    :return: The average losses, the metrics, all the outputs
    and the all the variances.
    """
    # initial message
    info("üîÑ Model's evaluation started...")

    # infer the batch
    (total_loss, loss_per_class,
     all_preds, all_targets, all_outputs, all_vars) = _infer_batch(
        model,
        loader,
        criterion,
        device
    )

    avg_loss, avg_loss_per_class = (
        _calculate_average_losses(
            total_loss,
            loss_per_class,
            len(loader)
        )
    )

    # debugging
    debug(f"‚öôÔ∏è Total predictions collected: {len(all_preds)}.")
    debug(f"‚öôÔ∏è Total targets: {len(all_targets)}.")

    # calculate the average of losses
    avg_loss = total_loss / len(loader)

    metrics = None
    if compute_metrics:
        # compute metrics
        metrics = _compute_metrics(
            all_targets,
            all_preds,
            all_outputs,
            config_settings
        )

        # show results
        info(f"üìâ Average Loss: {avg_loss}")

        info(f"üìâ Class Report per Class:")
        info(f"{metrics['class_report']}")

        info(f"\nConfusion Matrix:\n{np.array(metrics['confusion_matrix'])}")

        info(f"üìâ Top-k Accuracy: {metrics['top_k_accuracy']}")
        info(f"üìâ Kappa Statistic: {metrics['kappa_statistic']}")

        # show some plots
        plot_precision_recall_curve(
            all_targets,
            torch.stack(all_outputs).numpy(),
            config_settings.num_keys
        )

        plot_class_report(metrics["class_report"])

        plot_confusion_matrix(metrics["confusion_matrix"])

    # show a successful message
    info("üü¢ Model's evaluation completed.")

    return (avg_loss, avg_loss_per_class,
            metrics, all_outputs, all_vars)