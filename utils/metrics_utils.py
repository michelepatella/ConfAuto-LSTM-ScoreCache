import time
import numpy as np
import torch
from sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score
from utils.log_utils import info


def _calculate_top_k_accuracy(
        targets,
        outputs,
        config_settings
):
    """
    To calculate the top-k accuracy of the predictions.
    :param targets: The targets.
    :param outputs: The outputs of the model.
    :param config_settings: The configuration settings.
    :return: The top k-accuracy of the predictions.
    """
    # initial message
    info("ğŸ”„ Top-k accuracy computation started...")

    try:
        # prepare data
        outputs_tensor = torch.stack(outputs)

        # extract top-k predictions
        top_k_preds = (torch.topk(
            outputs_tensor,
            k=config_settings.top_k,
            dim=1
        ).indices.cpu().numpy())

        # initialize the no. of correct predictions
        correct = 0

        # count the correct predictions
        for i in range(len(targets)):

            # get the top-k predictions
            top_k_i = top_k_preds[i][:config_settings.top_k]

            # check if the target is contained into the
            # top-k predictions
            if targets[i] in top_k_i:
                correct += 1

        # calculate the accuracy
        accuracy = correct / len(targets)

    except (
            RuntimeError,
            IndexError,
            TypeError,
            ZeroDivisionError,
            ValueError
    ) as e:
        raise RuntimeError(f"âŒ Error while computing top-k accuracy: {e}.")

    # show a successful message
    info("ğŸŸ¢ Top-k accuracy computed.")

    return accuracy


def _calculate_kappa_statistic(
        targets,
        predictions
):
    """
    Method to calculate the kappa statistic.
    :param targets: The targets.
    :param predictions: The predictions coming from the model.
    :return: The kappa statistic.
    """
    # initial message
    info("ğŸ”„ Kappa statistic calculation started...")

    try:
        # calculate kappa statistic
        kappa = cohen_kappa_score(
            targets,
            predictions
        )
    except (
            ValueError,
            TypeError
    ) as e:
        raise RuntimeError(f"âŒ Error while calculating kappa statistic: {e}.")

    # show a successful message
    info("ğŸŸ¢ Kappa statistic calculated.")

    return kappa


def _compute_model_standalone_metrics(
        targets,
        predictions,
        outputs,
        config_settings
):
    """
    Method to compute metrics based on predictions and targets.
    :param targets: The targets.
    :param predictions: Predictions from model.
    :param outputs: Probabilities from model.
    :param config_settings: The configuration settings.
    :return: The computed metrics.
    """
    # initial message
    info("ğŸ”„ Metrics computation started...")

    try:
        # class-wise metrics
        class_report = classification_report(
            targets,
            predictions,
            output_dict=True,
            zero_division=0
        )

        # calculate the top-k accuracy
        top_k_accuracy = _calculate_top_k_accuracy(
            targets,
            outputs,
            config_settings
        )

        # compute the confusion matrix
        conf_matrix = confusion_matrix(
            targets,
            predictions
        )

        # calculate kappa statistic
        kappa_statistic = _calculate_kappa_statistic(
            targets,
            predictions
        )

    except (
            ValueError,
            TypeError
    ) as e:
        raise RuntimeError(f"âŒ Error while computing metrics: {e}.")

    # collect metrics
    metrics = {
        "class_report": class_report,
        "top_k_accuracy": top_k_accuracy,
        "confusion_matrix": conf_matrix.tolist(),
        "kappa_statistic": kappa_statistic
    }

    # show a successful message
    info("ğŸŸ¢ Metrics computed.")

    return metrics


def compute_eviction_mistake_rate(metrics_logger):
    """
    Method to compute eviction mistake rate.
    :param metrics_logger: The metrics logger.
    :return: The eviction mistake rate.
    """
    # initial message
    info("ğŸ”„ Eviction mistake rate calculation started...")
    print(metrics_logger.put_events)
    print(metrics_logger.access_events)
    print(metrics_logger.evicted_keys)
    print(metrics_logger.prefetch_predictions)
    try:
        # initialize data
        mistakes = 0
        total = len(metrics_logger.evicted_keys)

        # count mistakes due to wrongly evictions
        for key, eviction_time in metrics_logger.evicted_keys.items():
            future_accesses = [
                t for t in metrics_logger.access_events.get(key, [])
                if t > eviction_time
            ]
            if future_accesses:
                mistakes += 1

    except (
        AttributeError,
        TypeError,
        ZeroDivisionError
    ) as e:
        raise RuntimeError(f"âŒ Error while computing eviction mistake rate: {e}.")

    # show a successful message
    info("ğŸŸ¢ Eviction mistake rate computed.")

    return mistakes / total if total > 0 else 0


def compute_prefetch_hit_rate(
        metrics_logger,
        window_size
):
    """
    Method to compute prefetch hit rate.
    :param metrics_logger: The metrics logger.
    :param window_size: The window size.
    :return: The prefetch hit rate.
    """
    # initial message
    info("ğŸ”„ Prefetch hit rate calculation started...")

    # initialize data
    hits = 0
    total = 0

    try:
        # count prefetched keys have been hit
        for t, predicted_keys in metrics_logger.prefetch_predictions.items():
            if not isinstance(
                    predicted_keys,
                    (list, set, tuple)
            ):
                predicted_keys = [predicted_keys]
            total += len(predicted_keys)
            for key in predicted_keys:
                for access_time in metrics_logger.access_events.get(key, []):
                    if t < access_time <= t + window_size:
                        hits += 1
                        break
    except (
        AttributeError,
        TypeError,
        ValueError,
        ZeroDivisionError,
        NameError
    ) as e:
        raise RuntimeError(f"âŒ Error while computing prefetch hit rate: {e}.")

    # show a successful message
    info("ğŸŸ¢ Prefetch hit rate computed.")

    return hits / total if total > 0 else 0


def compute_ttl_mae(metrics_logger):
    """
    Method to compute TTL MAE.
    :param metrics_logger: The metrics logger.
    :return: The MAE.
    """
    # initial message
    info("ğŸ”„ TTL MAE calculation started...")

    errors = []

    try:
        # calculate MAE on TTL assigned
        for key, (put_time, predicted_ttl) in metrics_logger.put_events.items():
            actual_accesses = metrics_logger.access_events.get(key, [])
            if actual_accesses:
                last_use = max([
                    t for t in actual_accesses
                    if t >= put_time],
                    default=None
                )
                if last_use:
                    true_ttl = last_use - put_time
                    errors.append(abs(true_ttl - predicted_ttl))
    except (
        AttributeError,
        TypeError,
        ValueError,
        KeyError,
        NameError
    ) as e:
        raise RuntimeError(f"âŒ Error while computing TTL MAE: {e}.")

    # show a successful message
    info("ğŸŸ¢ TTL MAE computed.")

    return np.mean(errors) if errors else None


def calculate_hit_miss_rate(counters):
    """
    Method to calculate hit and miss rate.
    :param counters: A counter used while simulating a cache policy
    :return: The hit and miss rate in terms of %.
    """
    # initial message
    info("ğŸ”„ Hit and miss rate calculation started...")

    try:
        # calculate hit rate and miss rate in terms of %
        total = counters['hits'] + counters['misses']
        hit_rate = counters['hits'] / total * 100
        miss_rate = counters['misses'] / total * 100
    except (
            KeyError,
            ZeroDivisionError,
            TypeError,
            AttributeError
    ) as e:
        raise RuntimeError(f"âŒ Error while calculating hit and miss rate: {e}.")

    # show a successful message
    info("ğŸŸ¢ Hit and miss rate calculated.")

    return hit_rate, miss_rate


def calculate_cache_latency(
        start_time,
        latencies
):
    """
    Method to calculate cache latency.
    :param start_time: Start time of simulation.
    :param latencies: The past cache latencies.
    :return: The updated cache latencies.
    """
    # initial message
    info("ğŸ”„ Cache latency calculation started...")

    try:
        # at the end, calculate the latency
        end_time = time.perf_counter()
        latency = end_time - start_time
        latencies.append(latency)
    except (
            NameError,
            AttributeError,
            TypeError
    ) as e:
        raise RuntimeError(f"âŒ Error while calculating cache latency: {e}.")

    # show a successful message
    info("ğŸŸ¢ Cache latency calculated.")

    return latencies