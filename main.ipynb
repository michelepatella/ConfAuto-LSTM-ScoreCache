{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b3a980af97f3dc",
   "metadata": {},
   "source": [
    "# Confident-aware LSTM-based Intelligent Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20de7c1c24858ed",
   "metadata": {},
   "source": [
    "**This project explores the use of a Long Short-Term Memory (LSTM) network combined with Confidence Intervals (CIs) for an intelligent caching system implemented in Redis. The aim is to improve prefetching, TTL assignment, and eviction policies, providing a framework that outperforms traditional baseline strategies in terms of hit rate and miss rate.**\n",
    "\n",
    "This notebook is organized as follows:\n",
    "1. [**Configuration Settings**](#-configuration-settings)\n",
    "2. [**Data Generation**](#-data-generation)\n",
    "3. [**Data Preprocessing**](#-data-preprocessing)\n",
    "4. [**Validation**](#-validation)\n",
    "5. [**Training**](#-training)\n",
    "6. [**Testing**](#-testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdfa091e96cd6b5",
   "metadata": {},
   "source": [
    "## ⚙️ Configuration Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531737a2a446526",
   "metadata": {},
   "source": [
    "**Configuration settings** are centralized in `config.yaml` file. The file is composed by the following main sections:\n",
    "- `data`: Contains settings regard data generation (`distribution`, `access_pattern`, and `temporal_pattern`), about the data sequences (`sequence`), and the dataset (`dataset`).\n",
    "- `model`: Contains general model settings (`general`), and model parameter's settings (`params`).\n",
    "- `validation`: Contains settings about the Time Series Cross-Validation (`cross_validation`), Early Stopping (`early_stopping`), and hyperparameter search space (`search_space`).\n",
    "- `training`: Contains general training settings (`general`) and optimizer settings (`optimizer`).\n",
    "- `testing`: Contains general testing options (`general`).\n",
    "- `evaluation`: Contains settings about metrics (`top_k`).\n",
    "\n",
    "<br>\n",
    "\n",
    "## `data`\n",
    "### `data.distribution`\n",
    "- `data.distribution.seed`: The seed used for random generations (`int >= 0`).\n",
    "- `data.distribution.type`: The type of dataset to generate (`static` or `dynamic`).\n",
    "- `data.distribution.num_requests`: The number of requests to generate (i.e., the number of dataset's rows) (`int > 0`).\n",
    "- `data.distribution.num_keys`: The number of unique keys (`int > 1`, equal to `data.distribution.key_range.last_key - data.distribution.key_range.first_key + 1`).\n",
    "- `data.distribution.freq_windows`: The windows within to calculate the relative frequencies of the keys (`List[int > 0]`).\n",
    "\n",
    "#### `data.distribution.key_range`\n",
    "- `data.distribution.key_range.first_key`: The first key's ID to generate (`int < data.distribution.key_range.last_key`).\n",
    "- `data.distribution.key_range.last_key`: The last key's ID to generate (`int > data.distribution.key_range.first_key`).\n",
    "\n",
    "### `data.access_pattern`\n",
    "#### `data.access_pattern.zipf`\n",
    "- `data.access_pattern.zipf.apha`: The fixed Zipf parameter (for static data distribution) (`float > 0`).\n",
    "- `data.access_pattern.zipf.alpha_start`: The initial Zipf parameter (for dynamic data distribution) (`float > 0`, `<= data.access_pattern.zipf.alpha_end`).\n",
    "- `data.access_pattern.zipf.alpha_end`: The final Zipf parameter (for dynamic data distribution) (`float > 0`, `>= data.access_pattern.zipf.alpha_start`).\n",
    "- `data.access_pattern.zipf.time_steps`: The number of time steps to be considered while generating dynamic data distribution (`int > 0`).\n",
    "\n",
    "#### `data.access_pattern.locality`\n",
    "- `data.access_pattern.locality.prob`: The probability of local access pattern (`float in [0.0, 1.0]`).\n",
    "\n",
    "### `data.temporal_pattern`\n",
    "#### `data.temporal_pattern.burstiness`\n",
    "- `data.temporal_pattern.burstiness.burst_high`: Scaling factor used during the peak phase of burst (the lower the value the higher the number of requests) (`float > 0`, `< data.temporal_pattern.burstiness.burst_low`).\n",
    "- `data.temporal_pattern.burstiness.burst_low`: Scaling factor used outside the phase of burst (the higher the value the lower the number of requests) (`float > 0`, `> data.temporal_pattern.burstiness.burst_low`).\n",
    "- `data.temporal_pattern.burstiness.burst_every`: Defines the periodicity in terms of number of requests (e.g., each 100 requests there is a new burst period) (`int > 0`).\n",
    "- `data.temporal_pattern.burstiness.burst_peak`: Specifies the duration (in terms of requests) of the peak phase of burst (`int >= 0`).\n",
    "\n",
    "#### `data.temporal_pattern.periodic`\n",
    "- `data.temporal_pattern.periodic.base_scale`: The base frequency scaling factor (controls the request density baseline) (`int/float > 0`).\n",
    "- `data.temporal_pattern.periodic.amplitude`: The amplitude of the periodic variation (controls how much the request frequency oscillates) (`int/float >= 0`).\n",
    "\n",
    "### `data.sequence`\n",
    "- `data.sequence.len`: The length of the input sequence used as model input (`int > 0`).\n",
    "- `data.sequence.embedding_dim`: The dimensionality of the embedding space used to encode input keys (`int > 0`).\n",
    "\n",
    "### `data.dataset`\n",
    "- `data.dataset.training_perc`: The percentage of the dataset to be used as training set(`float in [0.0, 1.0]`).\n",
    "- `data.dataset.validation_perc`: The percentage of the training set to be used as validation set (`float in [0.0, 1.0)`).\n",
    "- `data.dataset.static_save_path`: Path where the static dataset will be saved (`str`).\n",
    "- `data.dataset.dynamic_save_path`: Path where the dynamic dataset will be saved (`str`).\n",
    "\n",
    "<br>\n",
    "\n",
    "## `model`\n",
    "### `model.general`\n",
    "- `model.general.num_features`: The number of features per input element (`int > 0`).\n",
    "- `model.general.save_path`: Path to save the trained model (`str`).\n",
    "\n",
    "### `model.params`\n",
    "- `model.params.hidden_size`: Number of units in the LSTM hidden layer (`int > 0`).\n",
    "- `model.params.num_layers`: Number of LSTM layers (`int > 0`).\n",
    "- `model.params.bias`: Whether to use bias weights in the LSTM (`bool`).\n",
    "- `model.params.batch_first`: Whether input/output tensors are provided as (batch, seq, feature) (`bool`).\n",
    "- `model.params.dropout`: Dropout probability between LSTM layers (`float in [0.0, 1.0)`).\n",
    "- `model.params.bidirectional`: Whether to use a bidirectional LSTM (`bool`).\n",
    "- `model.params.proj_size`: Size of the projection layer in LSTM (`int >= 0`, `<= model.params.hidden_size`).\n",
    "\n",
    "<br>\n",
    "\n",
    "## `validation`\n",
    "### `validation.cross_validation`\n",
    "- `validation.cross_validation.num_folds`: Number of folds to split the training set into during Time Series Cross-Validation (`int > 1`).\n",
    "- `validation.cross_validation.num_epochs`: Number of training epochs for each fold while validating the model (`int > 0`).\n",
    "\n",
    "### `validation.early_stopping`\n",
    "- `validation.early_stopping.patience`: Number of epochs to wait after the last improvement before stopping training (`int >= 0`).\n",
    "- `validation.early_stopping.delta`: Minimum change in validation loss to qualify as an improvement (`int/float >= 0`).\n",
    "\n",
    "### `validation.search_space.model.params`\n",
    "- `validation.search_space.model.params.hidden_size_range`: Non-empty list of possible values for `hidden_size` hyperparameter (`List[int > 0]`).\n",
    "- `validation.search_space.model.params.num_layers_range`: List of possible values for `num_layers` hyperparameter (`List[int > 0]`).\n",
    "- `validation.search_space.model.params.dropout_range`: Non-empty list of possible values for `dropout` hyperparameter (`List[float in [0.0, 1.0))`).\n",
    "\n",
    "### `validation.search_space.training.optimizer`\n",
    "- `validation.search_space.training.optimizer.learning_rate_range`: Non-empty list of learning rates to try (`List[float > 0]`).\n",
    "\n",
    "<br>\n",
    "\n",
    "## `training`\n",
    "### `training.general`\n",
    "- `training.general.num_epochs`: Number of epochs to train the final model (`int > 0`).\n",
    "- `training.general.batch_size`: Size of training batches (`int > 0`).\n",
    "\n",
    "### `training.optimizer`\n",
    "- `training.optimizer.type`: Type of optimizer used during training (`adam`, `adamw`, or `sgd`).\n",
    "- `training.optimizer.learning_rate`: Initial learning rate (`float > 0`).\n",
    "- `training.optimizer.weight_decay`: Weight decay factor (`float >= 0`).\n",
    "- `training.optimizer.momentum`: Momentum factor (if supported by optimizer) (`float in [0.0, 0.1]`).\n",
    "\n",
    "<br>\n",
    "\n",
    "## `testing`\n",
    "### `testing.general`\n",
    "- `testing.general.batch_size`: Size of batches during testing (`int > 0`).\n",
    "\n",
    "<br>\n",
    "\n",
    "## `evaluation`\n",
    "- `evaluation.top_k`: Value of `k` to compute Top-k accuracy (`int > 0`).\n",
    "\n",
    "<br>\n",
    "\n",
    "## `inference`\n",
    "### `inference.confidence_intervals`\n",
    "- `inference.confidence_intervals.confidence_level`: Confidence level used for confidence intervals calculation. (`float in [0.0, 1.0]`).\n",
    "\n",
    "### `inference.mc_dropout`\n",
    "- `inference.mc_dropout.num_samples`: The number of samples on which to apply MC dropout. (`int > 0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d805decaca7374",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T22:15:07.749400Z",
     "start_time": "2025-05-20T22:15:07.726057Z"
    }
   },
   "outputs": [],
   "source": [
    "from config import prepare_config\n",
    "\n",
    "config_settings = prepare_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0481e06a8ee89",
   "metadata": {},
   "source": [
    "## 🎲  Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e5eb3cc031af8",
   "metadata": {},
   "source": [
    "Before training, evaluating or running experiments, we need to **generate synthetic data** on which to work on. Data should reflect the nature of the accesses in real-world systems, which is often characterized by:\n",
    "- **Skewed popularity**: Some objects are more popular than others.\n",
    "- **Locality**: Some objects are correlated and are often accessed in sequence.\n",
    "- **Periodic access patterns**: Objects are accessed in recurring time intervals.\n",
    "- **Burstiness periods**: Sudden spikes in access frequency occur during short time periods.\n",
    "\n",
    "To simulate this behavior, we generate two types of synthetic access patterns:\n",
    "\n",
    "- **Spatial accesses**: Governed by Zipf distribution (the first keys are more likely to be used than later ones) and locality (after accessing to a certain key, the neighboring ones are more likely to be accessed also). The first access always targets a popular key, then the probability of accessing to a popular key is 30%. After the first access, the probability to access to a neighboring key is 70%.\n",
    "- **Temporal accesses**: Modelled by using a combination of periodic and bursty patterns. While some time periods follow predictable, recurring intervals, others exhibit intervals of sudden, high-frequency access.\n",
    "\n",
    "For doing more realistic experiments, we generate both a **static** and **dynamic dataset**. The first assumes key popularities are fixed over time (i.e., the Zipf parameter remains constant), whereas the second simulates changes in key popularities over time (i.e., the Zipf parameter varies).\n",
    "\n",
    "The final datasets are composed by the following columns:\n",
    "- `id`: The ID of the current request.\n",
    "- `delta_time`: The temporal distance between the current request and the previous one.\n",
    "- `freq_last_10`: The relative frequency of the current requested key in the last 10 accesses.\n",
    "- `freq_last_100`: The relative frequency of the current requested key in the last 100 accesses.\n",
    "- `freq_last_1000`: The relative frequency of the current requested key in the last 1000 accesses.\n",
    "- `request`: The ID of the requested key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae5bdba3327243",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from main import config_settings\n",
    "from data_generation import data_generation\n",
    "\n",
    "data_generation(config_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129b4d24fc8aa93",
   "metadata": {},
   "source": [
    "## 🧹 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65b9a7ee5ffc42a",
   "metadata": {},
   "source": [
    "**Data preprocessing** aims to prepare data for being used in the next steps. Three processes are performed here:\n",
    "- **Duplicates removal**: Removes all the duplicates based on one or more columns. In that case, `id` duplicates are removed from the dataset.\n",
    "- **Missing values removal**: Removes all rows having missing values from the dataset.\n",
    "- **Standardization**: Standardizes one or more columns, avoiding too large value distances between data in a given column. In that case, we standardize `id`, `delta_time`, `freq_last_10`, `freq_last_100`, and `freq_last_1000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ed2ac8b50a0bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import data_preprocessing\n",
    "\n",
    "data_preprocessing(config_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca20af10fe5fc945",
   "metadata": {},
   "source": [
    "## 🧭 Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af900baf47454ce",
   "metadata": {},
   "source": "**Validation** aims at finding the **best hyperparameters** to be used for training the final model. After defining the hyperparameter search space, we compute a **Grid Search** to explore all possible combinations. For each combination we perform a **Time Series Cross-Validation**, useful to avoid data leakage by preserving the temporal order of events. **Early Stopping** is applied while training on each fold, stopping the process when the validation loss starts to increase. Whenever a new hyperparameter combination achieves the best average validation loss seen so far, it is saved as the new best. At the end, we obtain the best hyperparameters (i.e., those yielding the lowest average validation loss)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d48318b7d596909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from validation import validation\n",
    "\n",
    "config_settings = validation(config_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c4800b1e3fa860",
   "metadata": {},
   "source": [
    "## 🧠 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4446d0f383566966",
   "metadata": {},
   "source": "After identifying the best hyperparameters, the **final model** is obtained by **training** using those optimal values. We reserve a percentage of training set as **validation set** and we define a higher number of epochs than those used for validating the model, for applying **Early Stopping** ensuring the model is trained over how many as possible as epochs, avoiding overfitting. As soon as a new model has proven to be the current best one (i.e., it returns the best validation loss), its weights are saved. At the end, obtained the best trained model, we save it."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866bbc3c59b8b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import training\n",
    "\n",
    "training(config_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c1e26af4230190",
   "metadata": {},
   "source": "##  🧪  Testing (Model Standalone)"
  },
  {
   "cell_type": "markdown",
   "id": "ab758b9da47efb3a",
   "metadata": {},
   "source": [
    "After training the model, we **evaluate** it standalone on the testing set. The evaluation **metrics** computed are:\n",
    "- **Average loss**: Weighted Cross Entropy.\n",
    "- **Average loss per class**: Weighted Cross Entropy.\n",
    "- **Class report**: Precision, Recall, and F1 for each class.\n",
    "- **Confusion matrix**: Summarizes the number of correct and incorrect prediction for each class.\n",
    "- **Top-k accuracy**: How many times the target is predicted in the first k most probable keys.\n",
    "- **Kappa statistic**: Compares the model with a random one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db662bb3120624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from testing import testing\n",
    "\n",
    "avg_loss, avg_loss_per_class, metrics, cost_perc = testing(config_settings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
