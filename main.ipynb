{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b3a980af97f3dc",
   "metadata": {},
   "source": [
    "# Confident-aware LSTM-based Intelligent Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20de7c1c24858ed",
   "metadata": {},
   "source": [
    "**This project explores the use of a Long Short-Term Memory (LSTM) network combined with Confidence Intervals (CIs) for an intelligent caching system. The aim is to improve prefetching, TTL assignment, and eviction policies, providing a framework that outperforms traditional baseline strategies in terms of hit rate and miss rate.**\n",
    "\n",
    "This notebook is organized as follows:\n",
    "- ‚öôÔ∏è Configuration Settings\n",
    "- üé≤ Data Generation\n",
    "- üßπ Data Preprocessing\n",
    "- üß≠ Validation\n",
    "- üß† Training\n",
    "- üß™ Evaluation (Model Standalone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdfa091e96cd6b5",
   "metadata": {},
   "source": "## ‚öôÔ∏è Configuration Settings"
  },
  {
   "cell_type": "markdown",
   "id": "531737a2a446526",
   "metadata": {},
   "source": [
    "**Configuration settings** are centralized in the `config.yaml` file. It is composed of the following sections:\n",
    "\n",
    "- `data`: Settings for data generation, access patterns, temporal patterns, sequences, and datasets.\n",
    "- `model`: General model and parameter settings.\n",
    "- `validation`: Time series cross-validation, early stopping, and hyperparameter search space.\n",
    "- `training`: Training and optimizer settings.\n",
    "- `testing`: Testing options.\n",
    "- `evaluation`: Evaluation metrics.\n",
    "- `inference`: Confidence intervals and MC dropout for uncertainty estimation.\n",
    "- `simulation`: Simulation of caching policies using either traditional or LSTM-based methods.\n",
    "\n",
    "<br>\n",
    "\n",
    "## `data`\n",
    "\n",
    "### `data.distribution`\n",
    "\n",
    "- `seed`: Random seed (`int >= 0`)\n",
    "- `type`: Dataset type (`static` or `dynamic`)\n",
    "- `num_requests`: Number of requests to generate (`int > 0`)\n",
    "- `num_keys`: Number of unique keys (`int > 1`)\n",
    "- `key_range.first_key`: First key ID (`int`)\n",
    "- `key_range.last_key`: Last key ID (`int > first_key`)\n",
    "\n",
    "### `data.access_pattern`\n",
    "\n",
    "#### `zipf`\n",
    "\n",
    "- `alpha`: Zipf parameter for static data (`float > 0`)\n",
    "- `alpha_start`: Initial alpha for dynamic data (`float > 0`)\n",
    "- `alpha_end`: Final alpha for dynamic data (`float > 0`)\n",
    "- `time_steps`: Number of time steps to transition alpha (`int > 0`)\n",
    "\n",
    "#### `access_behavior`\n",
    "\n",
    "- `repetition_interval`: Re-access interval for repeated keys (`int > 0`)\n",
    "- `repetition_offset`: Offset to apply when repeating (`int > 0`)\n",
    "- `toggle_interval`: Toggle interval for alternating accesses (`int > 0`)\n",
    "- `cycle_base`: Base length for cyclic scanning (`int > 0`)\n",
    "- `cycle_mod`: Modulus to vary cycle length (`int > 0`)\n",
    "- `cycle_divisor`: Divisor for cycle variability (`int > 0`)\n",
    "- `distortion_interval`: Interval for distorted history pattern (`int > 0`)\n",
    "- `noise_range`: Range of noise to apply to distorted memory ([min, max])\n",
    "- `memory_interval`: Interval for memory recall pattern (`int > 0`)\n",
    "- `memory_offset`: Offset to recall historical accesses (`int > 0`)\n",
    "\n",
    "### `data.temporal_pattern`\n",
    "\n",
    "#### `burstiness`\n",
    "\n",
    "- `burst_high`: Scaling factor for burst peaks (`float in [0, burst_low]`)\n",
    "- `burst_low`: Scaling factor for non-burst (`float > burst_high`)\n",
    "- `burst_hour_start`: Hour when burst starts (`int in [0, 23]`)\n",
    "- `burst_hour_end`: Hour when burst ends (`int in [0, 23]`)\n",
    "\n",
    "#### `periodic`\n",
    "\n",
    "- `base_scale`: Base frequency of periodic pattern (`int > 0`)\n",
    "- `amplitude`: Amplitude of the periodic variation (`int >= 0`)\n",
    "\n",
    "### `data.sequence`\n",
    "\n",
    "- `len`: Input sequence length (`int > 0`)\n",
    "- `embedding_dim`: Embedding dimension for keys (`int > 0`)\n",
    "\n",
    "### `data.dataset`\n",
    "\n",
    "- `training_perc`: Fraction of data for training (`float in [0.0, 1.0]`)\n",
    "- `validation_perc`: Fraction of training data for validation (`float in [0.0, 1.0)`)\n",
    "- `static_save_path`: Path to save static dataset (`string`)\n",
    "- `dynamic_save_path`: Path to save dynamic dataset (`string`)\n",
    "\n",
    "<br>\n",
    "\n",
    "## `model`\n",
    "\n",
    "### `model.general`\n",
    "\n",
    "- `num_features`: Number of input features for the model (`int > 0`)\n",
    "- `save_path`: Path to save trained model (`string`)\n",
    "\n",
    "### `model.params`\n",
    "\n",
    "- `hidden_size`: Size of LSTM hidden state (`int > 0`)\n",
    "- `num_layers`: Number of LSTM layers (`int > 0`)\n",
    "- `bias`: Whether to use bias in LSTM (`bool`)\n",
    "- `batch_first`: Use batch-first input format (`bool`)\n",
    "- `dropout`: Dropout between LSTM layers (`float in [0.0, 1.0)`)\n",
    "- `bidirectional`: Use bidirectional LSTM (`bool`)\n",
    "- `proj_size`: Size of projection layer (`int >= 0`)\n",
    "\n",
    "<br>\n",
    "\n",
    "## `validation`\n",
    "\n",
    "### `cross_validation`\n",
    "\n",
    "- `num_folds`: Number of cross-validation folds (`int > 1`)\n",
    "- `num_epochs`: Epochs per fold (`int > 0`)\n",
    "\n",
    "### `early_stopping`\n",
    "\n",
    "- `patience`: Epochs to wait for improvement (`int >= 0`)\n",
    "- `delta`: Minimum loss improvement (`float >= 0`)\n",
    "\n",
    "### `search_space`\n",
    "\n",
    "#### `model.params`\n",
    "\n",
    "- `hidden_size_range`: List of hidden sizes to try (`List[int > 0]`)\n",
    "- `num_layers_range`: List of layer counts to try (`List[int > 0]`)\n",
    "- `dropout_range`: List of dropout values (`List[float in [0.0, 1.0))`)\n",
    "\n",
    "#### `training.optimizer`\n",
    "\n",
    "- `learning_rate_range`: Learning rates to try (`List[float > 0]`)\n",
    "\n",
    "<br>\n",
    "\n",
    "## `training`\n",
    "\n",
    "### `training.general`\n",
    "\n",
    "- `num_epochs`: Total training epochs (`int > 0`)\n",
    "- `batch_size`: Batch size during training (`int > 0`)\n",
    "\n",
    "### `training.optimizer`\n",
    "\n",
    "- `type`: Optimizer type (`adam`, `adamw`, `sgd`)\n",
    "- `learning_rate`: Initial learning rate (`float > 0`)\n",
    "- `weight_decay`: L2 regularization (`float >= 0`)\n",
    "- `momentum`: Momentum for optimizer (if supported) (`float in [0.0, 1.0]`)\n",
    "\n",
    "### `training.early_stopping`\n",
    "\n",
    "- `patience`: Epochs without improvement before stopping (`int >= 0`)\n",
    "- `delta`: Minimum validation improvement to continue training (`float >= 0`)\n",
    "\n",
    "<br>\n",
    "\n",
    "## `testing`\n",
    "\n",
    "### `testing.general`\n",
    "\n",
    "- `batch_size`: Batch size during evaluation (`int > 0`)\n",
    "\n",
    "<br>\n",
    "\n",
    "## `evaluation`\n",
    "\n",
    "- `top_k`: `k` for Top-k accuracy metric (`int > 0`)\n",
    "\n",
    "<br>\n",
    "\n",
    "## `inference`\n",
    "\n",
    "### `confidence_intervals`\n",
    "\n",
    "- `confidence_level`: Confidence level for interval estimates (`float in [0.0, 1.0]`)\n",
    "\n",
    "### `mc_dropout`\n",
    "\n",
    "- `num_samples`: Number of stochastic passes for MC Dropout (`int > 0`)\n",
    "\n",
    "<br>\n",
    "\n",
    "## `simulation`\n",
    "\n",
    "### `general`\n",
    "\n",
    "- `cache_size`: Maximum number of keys in the cache (`int > 0`)\n",
    "\n",
    "### `traditional_cache`\n",
    "\n",
    "- `ttl`: Fixed TTL value for each key in the cache (`float > 0`)\n",
    "\n",
    "### `lstm_cache`\n",
    "\n",
    "- `prediction_interval`: How often the model makes predictions (`int > 0`)\n",
    "- `threshold_prob`: Minimum prediction probability to insert keys in cache (`float in [0.0, 1.0]`)\n",
    "- `threshold_ci`: Confidence interval threshold related to predictions to insert keys in cache (`float in [0.0, 1.0]`)\n",
    "- `ttl_base`: Base TTL for dynamic TTL calculation (`float > 0`)\n",
    "- `alpha`: Multiplicative factor applied to the predicted probability for TTL calculation (`float > 0`)\n",
    "- `beta`: Multiplicative factor applied to the confidence interval width for TTL adjustment (`float > 0`)"
   ]
  },
  {
   "cell_type": "code",
   "id": "75d805decaca7374",
   "metadata": {},
   "source": [
    "from config import prepare_config\n",
    "\n",
    "config_settings = prepare_config()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "24e0481e06a8ee89",
   "metadata": {},
   "source": "## üé≤ Data Generation"
  },
  {
   "cell_type": "markdown",
   "id": "316e5eb3cc031af8",
   "metadata": {},
   "source": [
    "Before training, evaluating or running experiments we need to **generate synthetic data**, reflecting the realistic nature of memory workload behaviours. We generate diverse **access patterns**, which determine the key to be accessed based on the time of the day:\n",
    "- **Repetition (05:00-09:00)**: Models short-term locality by periodically re-accessing recently used keys.\n",
    "- **Toggle (09:00-12:00)**: Simulates oscillating access behaviour.\n",
    "- **Cyclic scanning (12:00-18:00)**:  Models sequential scanning over subset of keys.\n",
    "- **Distorted history (18:00-23:00)**: Introduces noise to past accesses to simulate imprecise repetition or mutation.\n",
    "- **Memory call & Zipfian sampling (23:00-05:00)**: Alternates between accessing deep historical keys and sampling from a Zipf distribution.\n",
    "\n",
    "Inter-request times are modeled as a combination of **periodic** and **bursty** behaviours. Bursty activities occur on the mid-day (10:00-18:00).\n",
    "\n",
    "We generate 30,000 **requests** over a 25-day period for 30 distinct **keys**, organized in two **datasets** (each with two columns: `timestamp` and `request`, the latter indicating the ID of the requested key):\n",
    "- **Static dataset**: Assumes fixed key popularities over time, with the Zipf parameter set to 0.8.\n",
    "- **Dynamic dataset**: Models time-varying key popularity by linearly increasing the Zipf parameter from 0.5 to 2.0 over five steps."
   ]
  },
  {
   "cell_type": "code",
   "id": "47ae5bdba3327243",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from main import config_settings\n",
    "from data_generation import data_generation\n",
    "\n",
    "data_generation(config_settings)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "129b4d24fc8aa93",
   "metadata": {},
   "source": "## üßπ Data Preprocessing"
  },
  {
   "cell_type": "markdown",
   "id": "e65b9a7ee5ffc42a",
   "metadata": {},
   "source": [
    "**Data preprocessing** carries out two activities:\n",
    "- **Missing values removal**: Removes missing values from the dataset.\n",
    "- **Feature Engineering**: Aims to create two new columns‚Äî`sin_time` and `cos_time`‚Äîwhich replace the original `timestamp` with a trigonometric representation, enabling LSTM to better capture cyclical temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "id": "35ed2ac8b50a0bc0",
   "metadata": {},
   "source": [
    "from data_preprocessing import data_preprocessing\n",
    "\n",
    "data_preprocessing(config_settings)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ca20af10fe5fc945",
   "metadata": {},
   "source": "## üß≠ Validation"
  },
  {
   "cell_type": "markdown",
   "id": "5af900baf47454ce",
   "metadata": {},
   "source": [
    "**Validation** aims at finding the **best hyperparameters** to be used for training the final model. We define the **hyperparameter search space** as follows:\n",
    "- `hidden_size`: [128, 256]\n",
    "- `num_layers`: [2, 3]\n",
    "- `dropout`: [0.1, 0.3]\n",
    "- `learning_rate`: [0.001, 0.005]\n",
    "\n",
    "We compute a **Grid Search** over $2^4=16$ hyperparameter combinations. For each combination we perform a **10-fold Time Series Cross-Validation** on the training set (70% of the dataset), useful to avoid data leakage by preserving the temporal order of events.\n",
    "\n",
    "**Early Stopping** (`patience`=5, `delta`=0.0005) is applied while training on each fold (using **AdamW** as optimizer), stopping the process (involing 20 epochs at most) when the validation loss (calculated through the **weighted Cross Entropy Loss**) starts to increase.\n",
    "\n",
    "Whenever a new hyperparameter combination achieves the **best average validation loss** seen so far, it is saved as the new best. At the end, we obtain the best hyperparameters (i.e., those yielding the lowest average validation loss)."
   ]
  },
  {
   "cell_type": "code",
   "id": "7d48318b7d596909",
   "metadata": {},
   "source": [
    "from validation import validation\n",
    "\n",
    "config_settings = validation(config_settings)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c8c4800b1e3fa860",
   "metadata": {},
   "source": "## üß† Training"
  },
  {
   "cell_type": "markdown",
   "id": "4446d0f383566966",
   "metadata": {},
   "source": [
    "The final model is obtained by **training** with the optimal hyperparameters identified. We reserve 20% of training set as **validation set** and we define a higher number of epochs (500) than those used for validating the model. **Early Stopping** (`patience`=25, `delta`=0.0005) is applied ensuring the model is trained over how many as possible as epochs, avoiding overfitting.\n",
    "\n",
    "As soon as a new model has proven to be the current best one (i.e., it returns the best validation loss), its weights are saved. At the end, obtained the **best trained model**, we save it."
   ]
  },
  {
   "cell_type": "code",
   "id": "866bbc3c59b8b420",
   "metadata": {},
   "source": [
    "from training import training\n",
    "\n",
    "training(config_settings)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "61c1e26af4230190",
   "metadata": {},
   "source": "##  üß™ Evaluation (Model Standalone)"
  },
  {
   "cell_type": "markdown",
   "id": "ab758b9da47efb3a",
   "metadata": {},
   "source": [
    "After training the model, we **evaluate** it standalone on the testing set (30% of the dataset). The evaluation **metrics** computed are:\n",
    "- **Average loss** and **Average loss per class**.\n",
    "- **Class report**: Precision, Recall, and F1 for each class, Precision, and macro-average results.\n",
    "- **Confusion matrix**: Summarizes the number of correct and incorrect prediction for each class.\n",
    "- **Top-k accuracy**: How many times the target is predicted in the first `k=3` most probable keys.\n",
    "- **Kappa statistic**: Compares the model with a random one."
   ]
  },
  {
   "cell_type": "code",
   "id": "54db662bb3120624",
   "metadata": {},
   "source": [
    "from testing import testing\n",
    "\n",
    "avg_loss, avg_loss_per_class, metrics, cost_perc = testing(config_settings)\n",
    "\n",
    "# show results\n",
    "print(\"\\n\" + \"=\"*85)\n",
    "print(\" \" * 30 + \"Model Standalone Evaluation Report\")\n",
    "print(\"=\"*85 + \"\\n\")\n",
    "print(f\"Average Loss:       {avg_loss:.4f}\\n\")\n",
    "print(\"üìâ Class Report per Class:\")\n",
    "print(metrics['class_report'] + \"\\n\")\n",
    "print(f\"Top-k Accuracy:     {metrics['top_k_accuracy']:.4f}\")\n",
    "print(f\"Kappa Statistic:    {metrics['kappa_statistic']:.4f}\")\n",
    "print(\"\\n\" + \"=\"*85 + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  üñ•Ô∏è Simulation (Overall System)",
   "id": "671ae2201ebd375d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from simulation import simulate\n",
    "from cachetools import LRUCache, LFUCache, FIFOCache\n",
    "from simulation.LSTMCache import LSTMCache\n",
    "from simulation.RandomCache import RandomCache\n",
    "from simulation.CacheWrapper import CacheWrapper\n",
    "\n",
    "# setup cache strategies\n",
    "strategies = {\n",
    "    'LRU': CacheWrapper(LRUCache, config_settings),\n",
    "    'LFU': CacheWrapper(LFUCache, config_settings),\n",
    "    'FIFO': CacheWrapper(FIFOCache, config_settings),\n",
    "    'RANDOM': RandomCache(config_settings),\n",
    "    'LSTM': LSTMCache(config_settings),\n",
    "}\n",
    "\n",
    "# run simulation\n",
    "results = []\n",
    "for policy, cache in strategies.items():\n",
    "    result = simulate(cache, policy, config_settings)\n",
    "    results.append(result)\n",
    "\n",
    "# show results\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\" \" * 30 + \"Overall System Evaluation Report\")\n",
    "print(\"=\"*90 + \"\\n\")\n",
    "print(f\"{'Policy':<25} | {'Hit Rate (%)':>12} | {'Miss Rate (%)':>13}\")\n",
    "print(\"-\"*90)\n",
    "for res in results:\n",
    "    print(f\"{res['policy']:<25} | {res['hit_rate']:>12.2f} | {res['miss_rate']:>13.2f}\")\n",
    "print(\"\\n\" + \"=\"*90 + \"\\n\")"
   ],
   "id": "6f14a6874984cc44"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
